{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#for rapids\nimport sys\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for rapids\nimport cuml \nimport cudf as cd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import ElasticNet, Ridge\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport json \nimport pickle\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Loader\nfnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\nloading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\n\nlabels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ntrain_df = df[df[\"is_train\"] == True].copy()\n\n\n# used for training SVR better because SVR is sensitive to scale.\n# I initialy did not use this scale for trainig ridge and enet, but using this turned out to \\\n# have better cv for them too.\nFNC_SCALE = 1/500\n\ntrain_df[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE\n\n# excluded 'IC_20' features based on previous experiments using leave one out feature selection\nloading_features.remove('IC_20')\nfeatures = fnc_features + loading_features\n\ntargets = [\"age\", \"domain1_var1\",\"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for quickly testing cv and checking loss\ndef cv_test(model, train_df=train_df, test_df=test_df, features=features, targets=targets, folds=5):\n\n\n    kf = KFold(n_splits = folds)\n\n    losses = []\n\n\n    test_preds = pd.DataFrame()\n\n    for target in targets:\n\n        print(target)\n\n        test_pred_cv = pd.DataFrame()\n        loss = 0\n\n        train = train_df.loc[train_df[target].notnull()]\n        for fold, (train_ind, val_ind) in enumerate(kf.split(train)):\n                \n            \n            X_train, X_val = train.iloc[train_ind][features], train.iloc[val_ind][features]\n            y_train, y_val = train.iloc[train_ind][target], train.iloc[val_ind][target]\n\n            \n\n            \n            try:\n                model.fit(X_train, y_train)\n                pred = model.predict(X_val)\n                test_pred = model.predict(test_df[features])\n            except:\n                if target == 'age':\n                    c = 100\n                else:\n                    c = 10\n                model = SVR(C=c, cache_size=3000.0)\n                model.fit(cd.DataFrame(X_train), cd.Series(y_train))\n                pred = model.predict(cd.DataFrame(X_val))\n                pred = np.asarray(pred)\n                test_pred = np.asarray(model.predict(cd.DataFrame(test_df[features])))\n                \n\n            \n            loss+= metric(y_val, pred)/folds\n\n                \n            test_pred_cv = pd.concat([test_pred_cv, pd.Series(test_pred, name=f'{fold}')], axis=1)\n        test_mean = test_pred_cv.mean(axis=1)\n        test_preds[target] = test_mean\n\n        \n        \n        losses.append(loss)\n        print(loss, '\\n\\n')\n    \n    \n    final_score = losses[0]*0.30 + losses[1]*0.175 + losses[2]*0.175+ losses[3]*0.175+ losses[4]*0.175\n    print(final_score)\n    return(test_preds, losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from cuml import SVR\nsvr_test, svr_losses = cv_test(SVR())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nenet_test_preds, enet_losses = cv_test(ElasticNet(alpha=0.002, l1_ratio=0.99, max_iter=10000, \n                          normalize=True, selection='random', tol=1e-5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge_test_preds, ridge_losses = cv_test(Ridge(0.001))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the outputs from the above 3 lines\nridge_losses = [0.14432562046393557,\n 0.1518621332334051,\n 0.15199234197025407,\n 0.182002015241784,\n 0.1773679018606814]\n\nenet_losses = [0.1466418276102861,\n 0.15159310188684474,\n 0.15164378498766481,\n 0.1823087905152352,\n 0.17763578983352862]\n\n\nsvr_losses = [0.14452686132265957,\n 0.15536270536948454,\n 0.15517225770751503,\n 0.18654138255008823,\n 0.18036466196946171]\n\nimportance_mat = pd.DataFrame()\n\nfor i in range(5):\n    importance_mat[targets[i]] = pd.Series([1/ridge_losses[i], 1/enet_losses[i], 1/svr_losses[i]])\n    \nfor i in range(5):\n    sums = importance_mat[targets[i]].sum()\n    importance_mat[f'ridge_{targets[i]}'] = (1/ridge_losses[i])/sums\n    importance_mat[f'enet_{targets[i]}'] = (1/enet_losses[i])/sums\n    importance_mat[f'svr_{targets[i]}'] = (1/svr_losses[i])/sums\n\nimportance_mat = importance_mat.drop(targets, axis=1)\nimportance_mat = importance_mat.drop([0, 1], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking train and pred for test set\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport cudf as cd    #rapids\nfrom cuml import SVR #rapids\n\n#first, split for validation of meta model\n# meta_val, train = train_test_split(train_df)\n\n\nfeatures = fnc_features + loading_features # excluded IC_20\ntrain = train_df   #not validating meta model for now\ntest = test_df[features]\n\n\nfolds = 5\nkf = KFold(n_splits = folds, shuffle=True, random_state=0)#kfold cross val on train\n\n\nsub = pd.DataFrame()   # for storing meta preds on test set\n\n\nridge_test = pd.DataFrame()\nenet_test = pd.DataFrame()\nsvr_test = pd.DataFrame()\n\nmeta_train_df = pd.DataFrame()\n\nfor target in targets:\n    \n    print('\\n target : ', target)\n    \n    \n    ridge_test_preds = pd.DataFrame()\n    enet_test_preds = pd.DataFrame()\n    svr_test_preds = pd.DataFrame()\n    \n    meta_test_preds = pd.DataFrame()# for submission\n    meta_test_train = pd.DataFrame()# dataset used to train meta model on test set\n    \n    train = train.loc[train[target].notnull()] # using traindf without null target values\n    for fold, (train_ind, val_ind) in enumerate(kf.split(train)):\n        print('fold : ', fold)\n        \n        base_train_X, base_val_X = train.iloc[train_ind][features], train.iloc[val_ind][features]\n        base_train_y, base_val_y = train.iloc[train_ind][target], train.iloc[val_ind][target]\n        \n        \n        \n        # base models\n        ridge = Ridge(alpha=0.001)\n        enet = ElasticNet(alpha=0.002, l1_ratio=0.99, max_iter=10000, \n                          normalize=True, selection='random', tol=1e-5)\n        if target=='age':\n            c = 100\n        else:\n            c = 10\n        svr = SVR(C=c, cache_size=3000.0)\n        \n        \n        \n        #fit for base models\n        ridge.fit(base_train_X, base_train_y)\n        enet.fit(base_train_X, base_train_y)\n        svr.fit(cd.DataFrame(base_train_X), cd.Series(base_train_y))\n        \n        #predict on val for base models\n        ridge_pred = ridge.predict(base_val_X)\n        enet_pred = enet.predict(base_val_X)\n        svr_pred = svr.predict(cd.DataFrame(base_val_X))\n        svr_pred = np.asarray(svr_pred)\n        svr_pred = pd.Series(svr_pred)\n        \n        #predict on test for base models\n        ridge_test_pred = ridge.predict(test)\n        enet_test_pred = enet.predict(test)\n        svr_test_pred = svr.predict(cd.DataFrame(test))\n        svr_test_pred = np.asarray(svr_test_pred)\n        svr_test_pred = pd.Series(svr_test_pred)\n        \n        \n        \n        \n        #fit for meta model\n        r_w = importance_mat[f'ridge_{target}'].values  #weights \n        e_w = importance_mat[f'enet_{target}'].values\n        s_w = importance_mat[f'svr_{target}'].values\n        meta_train_X = ridge_pred*r_w + enet_pred*e_w + svr_pred*s_w\n        \n        meta_train_X = np.array(meta_train_X).reshape(-1, 1)\n        param = {'num_leaves':80, 'metric':'auc', 'objective':'regression'}\n        label = base_val_y\n        train_data = lgb.Dataset(meta_train_X, label=label)\n        meta_model = lgb.train(param, train_data)\n        \n        #predict for test set with meta model\n        meta_test_o = ridge_test_pred*r_w + enet_test_pred*e_w + svr_test_pred*s_w\n        meta_test = np.array(meta_test_o).reshape(-1, 1)\n        print('meta_shape', meta_test.shape)\n        meta_test_pred = meta_model.predict(meta_test)\n        \n        \n        \n        \n        #concat for taking mean later\n        ridge_test_preds = pd.concat([ridge_test_preds, pd.Series(ridge_test_pred)], axis=1)\n        enet_test_preds = pd.concat([enet_test_preds, pd.Series(enet_test_pred)], axis=1)\n        svr_test_preds = pd.concat([svr_test_preds, pd.Series(svr_test_pred)], axis=1)\n        meta_test_preds = pd.concat([meta_test_preds, pd.Series(meta_test_pred)], axis=1)\n        \n        meta_test_train = pd.concat([meta_test_train, pd.Series(np.asarray(meta_test_o))], axis=1)\n        \n        \n        \n        \n    #taking target wise mean\n    meta_mean = meta_test_preds.mean(axis=1)\n    ridge_mean = ridge_test_preds.mean(axis=1)\n    enet_mean = enet_test_preds.mean(axis=1)\n    svr_mean = svr_test_preds.mean(axis=1)\n    \n    meta_train_mean = meta_test_train.mean(axis=1)\n    \n    \n    \n    #saving test_predictions to dfs\n    sub[target] = meta_mean\n    ridge_test[target] = ridge_mean\n    enet_test[target] = enet_mean\n    svr_test[target] = svr_mean\n    meta_train_df[target] = meta_train_mean\n        \n       \n        \n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submit function for saving predictions to submit format csv\ndef submit(pred_df, test_df):\n    def to_sub(pred_df):\n        sub_df = pd.melt(pred_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=['Id'], value_name='Predicted')\n\n        sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\n        sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\n\n        #assert here is for debugging\n        assert sub_df.shape[0] == test_df.shape[0]*5\n\n        return sub_df\n\n    test_df.reset_index(drop=True, inplace=True)\n    pred_df.reset_index(drop=True, inplace=True)\n    pred_df['Id'] = test_df['Id'].astype('int')\n    sub_df = to_sub(pred_df)\n    sub_df.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit(sub, test_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}